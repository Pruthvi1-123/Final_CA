{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Basic machine learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# For handling imbalanced data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(filepath=\"customer_support_tickets.csv\"):\n",
    "    \"\"\"\n",
    "    Load the support ticket data and perform initial exploration\n",
    "    This helps us understand what we're working with\n",
    "    \"\"\"\n",
    "    print(\"\\nSTEP 1: Loading and exploring the data\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')\n",
    "        print(f\"Successfully loaded dataset with {len(df):,} tickets\")\n",
    "        \n",
    "        # Show basic information about the dataset\n",
    "        print(f\"Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "        # Display column names\n",
    "        print(f\"\\nColumns in dataset: {list(df.columns)}\")\n",
    "        \n",
    "        # Show first few rows to understand the data structure\n",
    "        print(f\"\\nFirst 3 rows of data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(f\"\\nMissing values per column:\")\n",
    "        missing_data = df.isnull().sum()\n",
    "        for col, missing in missing_data.items():\n",
    "            if missing > 0:\n",
    "                print(f\"  {col}: {missing} missing ({missing/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if missing_data.sum() == 0:\n",
    "            print(\"  No missing values found - excellent data quality\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f\"\\nDuplicate records: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file '{filepath}'\")\n",
    "        print(\"Please make sure the CSV file is in the same directory as this notebook\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the ticket text to make it easier for the computer to understand\n",
    "    Think of this like fixing spelling mistakes and removing unnecessary words\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase (so \"HELP\" and \"help\" are treated the same)\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove website links and emails (they're not useful for classification)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove placeholder text that's not useful\n",
    "    text = text.replace('product_purchase', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare our data for machine learning\n",
    "    This is like organizing your study materials before an exam\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 2: Cleaning and preparing the data...\")\n",
    "    \n",
    "    # Clean the ticket descriptions\n",
    "    df['cleaned_text'] = df['Ticket Description'].apply(clean_text)\n",
    "    \n",
    "    # Remove empty tickets (ones with no useful text)\n",
    "    df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    print(f\" Cleaned {len(df):,} tickets\")\n",
    "    \n",
    "    # Show some basic statistics about our text\n",
    "    avg_length = df['cleaned_text'].str.len().mean()\n",
    "    print(f\" Average ticket length: {avg_length:.0f} characters\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean our data\n",
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eddd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Convert text to numbers that the computer can understand\n",
    "    Think of this like translating from English to Math\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 3: Converting text to numbers...\")\n",
    "    \n",
    "    # Use TF-IDF to convert text to numbers\n",
    "    # TF-IDF finds the most important words in each ticket\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # Use the 1000 most important words\n",
    "        stop_words='english',  # Ignore common words like \"the\", \"and\"\n",
    "        ngram_range=(1, 2),  # Look at single words and word pairs\n",
    "        min_df=2  # Only use words that appear at least 2 times\n",
    "    )\n",
    "    \n",
    "    # Transform our text data\n",
    "    X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "    \n",
    "    print(f\" Created {X.shape[1]} features from text\")\n",
    "    print(f\" Feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X.toarray(), vectorizer  # Convert to regular array format\n",
    "\n",
    "# Create features from our text\n",
    "X, vectorizer = create_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(df):\n",
    "    \"\"\"\n",
    "    Prepare the things we want to predict (ticket type and priority)\n",
    "    This is like organizing the answer key for a test\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 4: Preparing labels...\")\n",
    "    \n",
    "    # Convert text labels to numbers\n",
    "    le_type = LabelEncoder()  # For ticket types\n",
    "    le_priority = LabelEncoder()  # For priorities\n",
    "    \n",
    "    # Transform the labels\n",
    "    y_type = le_type.fit_transform(df['Ticket Type'])\n",
    "    y_priority = le_priority.fit_transform(df['Ticket Priority'])\n",
    "    \n",
    "    # Combine both labels\n",
    "    y = np.column_stack((y_type, y_priority))\n",
    "    \n",
    "    # Show what we're predicting\n",
    "    print(f\" Ticket Types: {list(le_type.classes_)}\")\n",
    "    print(f\" Priority Levels: {list(le_priority.classes_)}\")\n",
    "    \n",
    "    # Show distribution of each type\n",
    "    print(\"\\n How many tickets of each type:\")\n",
    "    for i, ticket_type in enumerate(le_type.classes_):\n",
    "        count = (y_type == i).sum()\n",
    "        print(f\"   {ticket_type}: {count}\")\n",
    "    \n",
    "    return y, le_type, le_priority\n",
    "\n",
    "# Prepare our labels\n",
    "y, le_type, le_priority = prepare_labels(df) //"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

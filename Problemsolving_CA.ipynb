{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efd8a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data handling and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                           accuracy_score, precision_recall_fscore_support)\n",
    "\n",
    "# Text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "\n",
    "# Class imbalance handling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"IT Support Ticket Classification System\")\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"Starting analysis at:\", datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faec655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_explore_data(filepath=\"customer_support_tickets.csv\"):\n",
    "    \"\"\"\n",
    "    Load the support ticket data and perform initial exploration\n",
    "    This helps us understand what we're working with\n",
    "    \"\"\"\n",
    "    print(\"\\nSTEP 1: Loading and exploring the data\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "    try:\n",
    "        # Load the CSV file\n",
    "        df = pd.read_csv(filepath, encoding='utf-8')\n",
    "        print(f\"Successfully loaded dataset with {len(df):,} tickets\")\n",
    "        \n",
    "        # Show basic information about the dataset\n",
    "        print(f\"Dataset shape: {df.shape[0]} rows x {df.shape[1]} columns\")\n",
    "        print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "        # Display column names\n",
    "        print(f\"\\nColumns in dataset: {list(df.columns)}\")\n",
    "        \n",
    "        # Show first few rows to understand the data structure\n",
    "        print(f\"\\nFirst 3 rows of data:\")\n",
    "        print(df.head(3))\n",
    "        \n",
    "        # Check for missing values\n",
    "        print(f\"\\nMissing values per column:\")\n",
    "        missing_data = df.isnull().sum()\n",
    "        for col, missing in missing_data.items():\n",
    "            if missing > 0:\n",
    "                print(f\"  {col}: {missing} missing ({missing/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        if missing_data.sum() == 0:\n",
    "            print(\"  No missing values found - excellent data quality\")\n",
    "        \n",
    "        # Check for duplicates\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f\"\\nDuplicate records: {duplicates} ({duplicates/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file '{filepath}'\")\n",
    "        print(\"Please make sure the CSV file is in the same directory as this notebook\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load the data\n",
    "df = load_and_explore_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean the ticket text to make it easier for the computer to understand\n",
    "    Think of this like fixing spelling mistakes and removing unnecessary words\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase (so \"HELP\" and \"help\" are treated the same)\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove website links and emails (they're not useful for classification)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Remove special characters but keep letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove placeholder text that's not useful\n",
    "    text = text.replace('product_purchase', '')\n",
    "    \n",
    "    return text\n",
    "\n",
    "def prepare_data(df):\n",
    "    \"\"\"\n",
    "    Prepare our data for machine learning\n",
    "    This is like organizing your study materials before an exam\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 2: Cleaning and preparing the data...\")\n",
    "    \n",
    "    # Clean the ticket descriptions\n",
    "    df['cleaned_text'] = df['Ticket Description'].apply(clean_text)\n",
    "    \n",
    "    # Remove empty tickets (ones with no useful text)\n",
    "    df = df[df['cleaned_text'].str.len() > 0].reset_index(drop=True)\n",
    "    \n",
    "    print(f\" Cleaned {len(df):,} tickets\")\n",
    "    \n",
    "    # Show some basic statistics about our text\n",
    "    avg_length = df['cleaned_text'].str.len().mean()\n",
    "    print(f\" Average ticket length: {avg_length:.0f} characters\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean our data\n",
    "df = prepare_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6eddd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Convert text to numbers that the computer can understand\n",
    "    Think of this like translating from English to Math\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 3: Converting text to numbers...\")\n",
    "    \n",
    "    # Use TF-IDF to convert text to numbers\n",
    "    # TF-IDF finds the most important words in each ticket\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # Use the 1000 most important words\n",
    "        stop_words='english',  # Ignore common words like \"the\", \"and\"\n",
    "        ngram_range=(1, 2),  # Look at single words and word pairs\n",
    "        min_df=2  # Only use words that appear at least 2 times\n",
    "    )\n",
    "    \n",
    "    # Transform our text data\n",
    "    X = vectorizer.fit_transform(df['cleaned_text'])\n",
    "    \n",
    "    print(f\" Created {X.shape[1]} features from text\")\n",
    "    print(f\" Feature matrix shape: {X.shape}\")\n",
    "    \n",
    "    return X.toarray(), vectorizer  # Convert to regular array format\n",
    "\n",
    "# Create features from our text\n",
    "X, vectorizer = create_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1915cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_labels(df):\n",
    "    \"\"\"\n",
    "    Prepare the things we want to predict (ticket type and priority)\n",
    "    This is like organizing the answer key for a test\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 4: Preparing labels...\")\n",
    "    \n",
    "    # Convert text labels to numbers\n",
    "    le_type = LabelEncoder()  # For ticket types\n",
    "    le_priority = LabelEncoder()  # For priorities\n",
    "    \n",
    "    # Transform the labels\n",
    "    y_type = le_type.fit_transform(df['Ticket Type'])\n",
    "    y_priority = le_priority.fit_transform(df['Ticket Priority'])\n",
    "    \n",
    "    # Combine both labels\n",
    "    y = np.column_stack((y_type, y_priority))\n",
    "    \n",
    "    # Show what we're predicting\n",
    "    print(f\" Ticket Types: {list(le_type.classes_)}\")\n",
    "    print(f\" Priority Levels: {list(le_priority.classes_)}\")\n",
    "    \n",
    "    # Show distribution of each type\n",
    "    print(\"\\n How many tickets of each type:\")\n",
    "    for i, ticket_type in enumerate(le_type.classes_):\n",
    "        count = (y_type == i).sum()\n",
    "        print(f\"   {ticket_type}: {count}\")\n",
    "    \n",
    "    return y, le_type, le_priority\n",
    "\n",
    "# Prepare our labels\n",
    "y, le_type, le_priority = prepare_labels(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586a05a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_balance_data(X, y):\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets, then balance classes\n",
    "    This is like dividing your study material into practice and final exam\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 5: Splitting and balancing data...\")\n",
    "    \n",
    "    # Split the data (80% for training, 20% for testing)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\" Training data: {X_train.shape[0]} tickets\")\n",
    "    print(f\" Testing data: {X_test.shape[0]} tickets\")\n",
    "    \n",
    "    # Handle imbalanced classes using SMOTE\n",
    "    # SMOTE creates synthetic examples of minority classes\n",
    "    print(\"\\n Balancing classes...\")\n",
    "    \n",
    "    smote = SMOTE(random_state=42)\n",
    "    \n",
    "    # Balance the ticket types\n",
    "    X_train_balanced, y_train_type_balanced = smote.fit_resample(X_train, y_train[:, 0])\n",
    "    \n",
    "    # For priorities, we'll use a simple approach\n",
    "    # Map the balanced ticket types to their most common priorities\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(X_train)\n",
    "    \n",
    "    # Find closest original samples for balanced data\n",
    "    distances, indices = nn.kneighbors(X_train_balanced)\n",
    "    y_train_priority_balanced = y_train[:, 1][indices.flatten()]\n",
    "    \n",
    "    # Combine balanced labels\n",
    "    y_train_balanced = np.column_stack((y_train_type_balanced, y_train_priority_balanced))\n",
    "    \n",
    "    print(f\" Balanced training data: {X_train_balanced.shape[0]} tickets\")\n",
    "    \n",
    "    return X_train_balanced, X_test, y_train_balanced, y_test\n",
    "\n",
    "# Split and balance our data  \n",
    "X_train, X_test, y_train, y_test = split_and_balance_data(X, y)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train our machine learning model\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 6: Training the machine learning model...\")\n",
    "    \n",
    "    # Create a model that can predict both ticket type and priority\n",
    "    model = MultiOutputClassifier(\n",
    "        LogisticRegression(max_iter=1000, random_state=42)\n",
    "    )\n",
    "    \n",
    "    # Train the model (this is where the learning happens)\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\" Model trained successfully in {training_time:.2f} seconds\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Train our model\n",
    "model = train_model(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aab111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, le_type, le_priority):\n",
    "    \"\"\"\n",
    "    Test how well our model performs\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 7: Evaluating model performance...\")\n",
    "    \n",
    "    # Make predictions on test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy for each task\n",
    "    type_accuracy = accuracy_score(y_test[:, 0], y_pred[:, 0])\n",
    "    priority_accuracy = accuracy_score(y_test[:, 1], y_pred[:, 1])\n",
    "    \n",
    "    print(f\" Ticket Type Accuracy: {type_accuracy:.3f} ({type_accuracy*100:.1f}%)\")\n",
    "    print(f\" Priority Accuracy: {priority_accuracy:.3f} ({priority_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Show detailed results for ticket types\n",
    "    print(\"\\n Detailed Results for Ticket Types:\")\n",
    "    print(classification_report(y_test[:, 0], y_pred[:, 0], \n",
    "                              target_names=le_type.classes_, zero_division=0))\n",
    "    \n",
    "    # Show detailed results for priorities\n",
    "    print(\"\\n Detailed Results for Priorities:\")\n",
    "    print(classification_report(y_test[:, 1], y_pred[:, 1], \n",
    "                              target_names=le_priority.classes_, zero_division=0))\n",
    "    \n",
    "    # Create confusion matrices (visual representation of results)\n",
    "    plot_confusion_matrices(y_test, y_pred, le_type, le_priority)\n",
    "    \n",
    "    return y_pred\n",
    "\n",
    "def plot_confusion_matrices(y_test, y_pred, le_type, le_priority):\n",
    "    \"\"\"\n",
    "    Create visual charts showing where the model makes mistakes\n",
    "    \"\"\"\n",
    "    print(\"\\n Creating confusion matrices...\")\n",
    "    \n",
    "    # Create side-by-side plots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Ticket Type confusion matrix\n",
    "    cm_type = confusion_matrix(y_test[:, 0], y_pred[:, 0])\n",
    "    sns.heatmap(cm_type, annot=True, fmt='d', ax=ax1, cmap='Blues',\n",
    "                xticklabels=le_type.classes_, yticklabels=le_type.classes_)\n",
    "    ax1.set_title('Ticket Type Predictions')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Actual')\n",
    "    \n",
    "    # Priority confusion matrix\n",
    "    cm_priority = confusion_matrix(y_test[:, 1], y_pred[:, 1])\n",
    "    sns.heatmap(cm_priority, annot=True, fmt='d', ax=ax2, cmap='Oranges',\n",
    "                xticklabels=le_priority.classes_, yticklabels=le_priority.classes_)\n",
    "    ax2.set_title('Priority Level Predictions')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate our model\n",
    "y_pred = evaluate_model(model, X_test, y_test, le_type, le_priority)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c39daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_new_ticket(text, model, vectorizer, le_type, le_priority):\n",
    "    \"\"\"\n",
    "    Predict the type and priority of a new ticket\n",
    "    This is like using our trained model on new data\n",
    "    \"\"\"\n",
    "    # Clean the input text\n",
    "    cleaned_text = clean_text(text)\n",
    "    \n",
    "    # Convert text to numbers using our trained vectorizer\n",
    "    text_features = vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(text_features)\n",
    "    probabilities = model.predict_proba(text_features)\n",
    "    \n",
    "    # Convert numbers back to text labels\n",
    "    predicted_type = le_type.inverse_transform([prediction[0][0]])[0]\n",
    "    predicted_priority = le_priority.inverse_transform([prediction[0][1]])[0]\n",
    "    \n",
    "    # Get confidence scores\n",
    "    type_confidence = probabilities[0].max()\n",
    "    priority_confidence = probabilities[1].max()\n",
    "    \n",
    "    return {\n",
    "        'type': predicted_type,\n",
    "        'priority': predicted_priority,\n",
    "        'type_confidence': type_confidence,\n",
    "        'priority_confidence': priority_confidence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_examples():\n",
    "    \"\"\"\n",
    "    Test our model with some example tickets\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 8: Testing with example tickets...\")\n",
    "    \n",
    "    # Example tickets to test\n",
    "    example_tickets = [\n",
    "        \"My email is not working and I cannot send any messages\",\n",
    "        \"I need to cancel my subscription and get a refund\",\n",
    "        \"The software keeps crashing when I try to save files\",\n",
    "        \"I forgot my password and need help logging in\",\n",
    "        \"The printer is not connecting to my computer\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n Predictions for example tickets:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, ticket in enumerate(example_tickets, 1):\n",
    "        result = predict_new_ticket(ticket, model, vectorizer, le_type, le_priority)\n",
    "        \n",
    "        print(f\"\\n Example {i}:\")\n",
    "        print(f\"Ticket: '{ticket}'\")\n",
    "        print(f\"  Type: {result['type']} (confidence: {result['type_confidence']:.2f})\")\n",
    "        print(f\" Priority: {result['priority']} (confidence: {result['priority_confidence']:.2f})\")\n",
    "\n",
    "# Test with examples\n",
    "test_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f7f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model():\n",
    "    \"\"\"\n",
    "    Save our trained model so we can use it later\n",
    "    \"\"\"\n",
    "    print(\"\\n STEP 9: Saving the model...\")\n",
    "    \n",
    "    import joblib\n",
    "    \n",
    "    # Save all the components we need\n",
    "    model_components = {\n",
    "        'model': model,\n",
    "        'vectorizer': vectorizer,\n",
    "        'le_type': le_type,\n",
    "        'le_priority': le_priority\n",
    "    }\n",
    "    \n",
    "    joblib.dump(model_components, 'ticket_classifier_model.pkl')\n",
    "    print(\" Model saved as 'ticket_classifier_model.pkl'\")\n",
    "\n",
    "# Save our model\n",
    "save_model()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
